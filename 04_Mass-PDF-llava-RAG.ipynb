{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be32f03-7b01-4bd0-abb1-728bf4b5284b",
   "metadata": {},
   "source": [
    "# Preliminaries + Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa46804-4309-41ff-85ba-904a05e11b6d",
   "metadata": {},
   "source": [
    "These instructions are for Python 3.10\n",
    "### Fix Pillow\n",
    "* `cd /usr/lib/python3/dist-packages`\n",
    "* `sudo rm -rf pillow-10.2.0.egg-info`\n",
    "* `python3.10 -m pip install Pillow --user`\n",
    "### Install Ollama\n",
    "* `cd /tmp`\n",
    "* `curl -fsSL https://ollama.com/install.sh | sh`\n",
    "* Test, Optional (2GB download): `ollama run llama3.2`, Type `/bye` when done\n",
    "### Install Langchain\n",
    "* `python3.10 -m pip install langchain langchain_community langchain_chroma langchain_ollama llama-index-legacy langchain-unstructured --user`\n",
    "* `python3.10 -m pip install pypdf \"unstructured[pdf]\" --user` , This needed to be separate for some reason.\n",
    "### Install SQLite ( >= 3.35.0 required, This will install 3.46 )\n",
    "* `sudo apt install libreadline-dev python3.10-dev`\n",
    "* `wget https://sqlite.org/2024/sqlite-autoconf-3460100.tar.gz`\n",
    "* `tar -xvf sqlite-autoconf-3460100.tar.gz && cd sqlite-autoconf-3460100`\n",
    "* `./configure`\n",
    "* `make`\n",
    "* `sudo make install`\n",
    "* `python3.10 -m pip uninstall pysqlite3`\n",
    "* `python3.10 -m pip install pysqlite3-binary --user`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8accfe-d20f-49f8-a171-5dea85fe90c3",
   "metadata": {},
   "source": [
    "# Build/Expand Document Database + Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8be64-1186-4fc2-a5e6-ba4b0ed9249f",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a98174-c799-4dc2-ba0c-610379602900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ, path\n",
    "import time, sys, json\n",
    "now = time.time\n",
    "\n",
    "from utils import RAG_State\n",
    "\n",
    "\n",
    "environ[\"_RAG_STATE_PATH\"] = \"data/state.json\"\n",
    "\n",
    "RAGstate = RAG_State.load_state( environ[\"_RAG_STATE_PATH\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32057c-f5bd-4aea-a78f-9bb14b28f510",
   "metadata": {},
   "source": [
    "## Copy PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8219dcd5-258d-4463-bb82-bcafaa104e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path, makedirs\n",
    "# from aa_scrape_PDF import copy_pdfs\n",
    "\n",
    "environ[\"_RAG_PDF_SOURCE\"] = \"/media/james/FILEPILE/$_Robust_Planning/Literature/References/storage\"\n",
    "environ[\"_RAG_PDF_DESTIN\"] = \"data/input/pdf\"\n",
    "environ[\"_RAG_PDF_ERROR\"]  = \"data/input/BAD_PDF\"\n",
    "environ[\"_RAG_PAGE_DESTN\"] = \"data/input/pages\"\n",
    "environ[\"_RAG_VERBOSE\"]    =    \"\"\n",
    "environ[\"_RAG_DOC_ADD\"]    =   \"400\" #\"25\" #\"200\"\n",
    "environ[\"_RAG_DOC_LIMIT\"]  = \"20000\"\n",
    "environ[\"_RAG_DOC_DBASE\"]  = \"lit_pdf\"\n",
    "environ[\"_RAG_RUN_QUERY\"]  = \"True\" #\"\" #\"True\"\n",
    "\n",
    "if not path.exists( environ[\"_RAG_PDF_ERROR\"] ):\n",
    "    makedirs( environ[\"_RAG_PDF_ERROR\"] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335fa2a-8447-41b8-aca7-c2b31156624d",
   "metadata": {},
   "source": [
    "## Determine if more docs will be loaded this session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1794aee4-6208-43eb-b8e0-3e13ff3c4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Loading the vector store sometimes spews warnings\n",
    "\n",
    "__import__('pysqlite3')\n",
    "sys.modules['sqlite3'] = sys.modules.pop( 'pysqlite3' )\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "persistent_client = chromadb.PersistentClient();\n",
    "collection        = persistent_client.get_or_create_collection( environ[\"_RAG_DOC_DBASE\"] );\n",
    "\n",
    "pdfs_drct   = environ[\"_RAG_PDF_DESTIN\"]\n",
    "eror_drct   = environ[\"_RAG_PDF_ERROR\" ]\n",
    "fNames      = [item for item in os.listdir( pdfs_drct ) if (str( item ).split('.')[-1].lower() == 'pdf')]\n",
    "\n",
    "environ[\"_RAG_PDF_COUNT\"   ] = str( len( fNames ) )\n",
    "environ[\"_RAG_DOCDB_COUNT\" ] = str( collection.count() )\n",
    "environ[\"_RAG_DOCDB_REMAIN\"] = str( min( int(environ[\"_RAG_DOC_LIMIT\"])-len(RAGstate.docPaths), int(environ[\"_RAG_DOC_ADD\"]) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9db8f2-bada-49c1-99fd-acfb34e4157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10684 input PDFs exist!\n",
      "234766 vector records exist!\n",
      "400 files will be copied!\n",
      "...........................................................................................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "from utils import copy_pdfs\n",
    "\n",
    "print( f\"{environ['_RAG_PDF_COUNT' ]} input PDFs exist!\" )\n",
    "print( f\"{environ['_RAG_DOCDB_COUNT' ]} vector records exist!\" )\n",
    "print( f\"{environ['_RAG_DOCDB_REMAIN']} files will be copied!\" )\n",
    "\n",
    "copy_pdfs( environ[\"_RAG_PDF_SOURCE\"], environ[\"_RAG_PDF_DESTIN\"], \n",
    "           int(environ['_RAG_DOCDB_REMAIN']), verbose = environ[\"_RAG_VERBOSE\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b97eeb-bbeb-4b40-b0a2-61b8f7643707",
   "metadata": {},
   "source": [
    "## Load PDFs by page chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393314ba-7acb-46bf-bfb8-640e72407061",
   "metadata": {},
   "source": [
    "### https://python.langchain.com/docs/how_to/document_loader_pdf/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c5b7d6-c463-4917-89e4-f1aa6c040058",
   "metadata": {},
   "source": [
    "* `python3.10 -m pip install pypdf langchain-unstructured \"unstructured[pdf]\" --user`\n",
    "* `apt install tesseract-ocr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a906ea0-8714-47bf-99f8-124dc9862b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_unique_metadata_by_key( key ):\n",
    "    \"\"\" Return a list of unqiue metadata values from the vector store by `key` \"\"\"\n",
    "    global collection\n",
    "    unique = set()\n",
    "    for result in collection.get()['metadatas']:\n",
    "       if key in result:\n",
    "           unique.add( result[key] )\n",
    "    return list( unique )\n",
    "\n",
    "\n",
    "def find_unread_PDFs_at_input():\n",
    "    \"\"\" Return a list of PDF paths that do NOT have pages in the vector store \"\"\"\n",
    "    global RAGstate\n",
    "    inputPDFs = collect_unique_metadata_by_key( 'source' )\n",
    "    rtnPaths  = list()\n",
    "    for pdf in inputPDFs:\n",
    "        if pdf not in RAGstate['libDocs']:\n",
    "            rtnPaths.append( pdf )\n",
    "    return rtnPaths\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a2cca2-b617-4243-9e82-82e26de66e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10684 candidate files!\n",
      "ERROR:Invalid Elementary Object starting with b'\\x03' @0: b'\\x03\\xff', Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_92159/1697936586.py\", line 26, in <module>\n",
      "    async for page in loader.alazy_load():\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_core/document_loaders/base.py\", line 83, in alazy_load\n",
      "    doc = await run_in_executor(None, next, iterator, done)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 588, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 579, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py\", line 257, in lazy_load\n",
      "    yield from self.parser.parse(blob)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_core/document_loaders/base.py\", line 127, in parse\n",
      "    return list(self.lazy_parse(blob))\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py\", line 125, in lazy_parse\n",
      "    yield from [\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py\", line 127, in <listcomp>\n",
      "    page_content=_extract_text_from_page(page=page)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py\", line 117, in _extract_text_from_page\n",
      "    return page.extract_text(\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/_page.py\", line 2391, in extract_text\n",
      "    return self._extract_text(\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/_page.py\", line 2086, in _extract_text\n",
      "    for operands, operator in content.operations:\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/generic/_data_structures.py\", line 1421, in operations\n",
      "    self._parse_content_stream(BytesIO(self._data))\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/generic/_data_structures.py\", line 1323, in _parse_content_stream\n",
      "    operands.append(read_object(stream, None, self.forced_encoding))\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/generic/_data_structures.py\", line 1494, in read_object\n",
      "    raise PdfReadError(\n",
      "pypdf.errors.PdfReadError: Invalid Elementary Object starting with b'\\x03' @0: b'\\x03\\xff'\n",
      "ERROR:Cannot read an empty file, Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_92159/1697936586.py\", line 26, in <module>\n",
      "    async for page in loader.alazy_load():\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_core/document_loaders/base.py\", line 83, in alazy_load\n",
      "    doc = await run_in_executor(None, next, iterator, done)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 588, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 579, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py\", line 257, in lazy_load\n",
      "    yield from self.parser.parse(blob)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_core/document_loaders/base.py\", line 127, in parse\n",
      "    return list(self.lazy_parse(blob))\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py\", line 123, in lazy_parse\n",
      "    pdf_reader = pypdf.PdfReader(pdf_file_obj, password=self.password)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/_reader.py\", line 135, in __init__\n",
      "    self._initialize_stream(stream)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/_reader.py\", line 156, in _initialize_stream\n",
      "    self.read(stream)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/_reader.py\", line 577, in read\n",
      "    self._basic_validation(stream)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/_reader.py\", line 643, in _basic_validation\n",
      "    raise EmptyFileError(\"Cannot read an empty file\")\n",
      "pypdf.errors.EmptyFileError: Cannot read an empty file\n",
      "ERROR:Stream has ended unexpectedly, Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_92159/1697936586.py\", line 26, in <module>\n",
      "    async for page in loader.alazy_load():\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_core/document_loaders/base.py\", line 83, in alazy_load\n",
      "    doc = await run_in_executor(None, next, iterator, done)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 588, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 579, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py\", line 257, in lazy_load\n",
      "    yield from self.parser.parse(blob)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_core/document_loaders/base.py\", line 127, in parse\n",
      "    return list(self.lazy_parse(blob))\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py\", line 125, in lazy_parse\n",
      "    yield from [\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py\", line 127, in <listcomp>\n",
      "    page_content=_extract_text_from_page(page=page)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py\", line 117, in _extract_text_from_page\n",
      "    return page.extract_text(\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/_page.py\", line 2391, in extract_text\n",
      "    return self._extract_text(\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/_page.py\", line 2086, in _extract_text\n",
      "    for operands, operator in content.operations:\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/generic/_data_structures.py\", line 1421, in operations\n",
      "    self._parse_content_stream(BytesIO(self._data))\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/generic/_data_structures.py\", line 1323, in _parse_content_stream\n",
      "    operands.append(read_object(stream, None, self.forced_encoding))\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/generic/_data_structures.py\", line 1464, in read_object\n",
      "    return ArrayObject.read_from_stream(stream, pdf, forced_encoding)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/generic/_data_structures.py\", line 267, in read_from_stream\n",
      "    arr.append(read_object(stream, pdf, forced_encoding))\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/generic/_data_structures.py\", line 1468, in read_object\n",
      "    return read_string_from_stream(stream, forced_encoding)\n",
      "  File \"/home/james/.local/lib/python3.10/site-packages/pypdf/generic/_utils.py\", line 72, in read_string_from_stream\n",
      "    raise PdfStreamError(STREAM_TRUNCATED_PREMATURELY)\n",
      "pypdf.errors.PdfStreamError: Stream has ended unexpectedly\n",
      "\n",
      "Read 0 pages in 0.07 minutes! (Skipped 10681 existing PDFs.)\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, traceback, sys\n",
    "from collections import deque\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pages = deque() # Fast append\n",
    "\n",
    "\n",
    "# needToParse = find_unread_PDFs_at_input()\n",
    "# fNames.extend( needToParse )\n",
    "\n",
    "print( f\"There are {len(fNames)} candidate files!\" )\n",
    "\n",
    "if len( fNames ) > 0:\n",
    "    bgn    = now()\n",
    "    lastLn = 0\n",
    "    Nexist = 0\n",
    "    \n",
    "    for i, fNam in enumerate( fNames ):\n",
    "        # file_path = str( path.join( pdfs_drct, fNam ) ) # fNam # str( path.join( pdfs_drct, fNam ) )\n",
    "        file_path = fr\"{path.join( pdfs_drct, fNam )}\" # fNam # str( path.join( pdfs_drct, fNam ) )\n",
    "        # file_path = file_path.replace( ' ', '\\\\ ' )\n",
    "        # file_path = file_path.replace( ' ', '\\\\ ' )\n",
    "        if file_path not in RAGstate.docPaths:\n",
    "            try:\n",
    "                loader    = PyPDFLoader( file_path )\n",
    "                async for page in loader.alazy_load():\n",
    "                    pages.append( page )\n",
    "                print( f\"{i+1}:{len(pages)-lastLn}:{len(pages)}\", end = ', ', flush = True )\n",
    "                lastLn = len(pages)\n",
    "            except Exception as e:\n",
    "                print( f\"ERROR:{e}\", end = ', ', flush = True )\n",
    "                traceback.print_exc( file = sys.stdout )\n",
    "                try:\n",
    "                    # errr_path = str(path.join( eror_drct, fNam ))\n",
    "                    errr_path = fr\"{path.join( eror_drct, fNam )}\"\n",
    "                    # errr_path = errr_path.replace( ' ', '\\\\ ' )\n",
    "                    shutil.move( file_path, errr_path )\n",
    "                except Exception as e:\n",
    "                    print( f\"FAILED to move {file_path} --to-> {path.join( eror_drct, fNam )}\" )\n",
    "            except asyncio.CancelledError as e:\n",
    "                print( f\"Load operation cancelled by user\" )\n",
    "                raise e\n",
    "        else:\n",
    "            Nexist += 1\n",
    "                    \n",
    "    print()\n",
    "    pages = list( pages )\n",
    "    print( f\"Read {len(pages)} pages in {(now()-bgn)/60.0:.2f} minutes! (Skipped {Nexist} existing PDFs.)\" )\n",
    "\n",
    "RAGstate.save_state( environ[\"_RAG_STATE_PATH\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e216559a-5e4b-4d21-a8ff-6d796e0c5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"{pages[0].metadata}\\n\")\n",
    "# print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0be2b0-5b1c-4838-b4db-62b518afeac4",
   "metadata": {},
   "source": [
    "## Load the text embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35da1790-0281-4af3-8b81-88560fc16a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to save 'all-minilm'.\n",
      "This will spew a lot of text on the first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 797b70c4edf8... 100% ▕████████████████▏  45 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 85011998c600... 100% ▕████████████████▏   16 B                         \n",
      "pulling 548455b72658... 100% ▕████████████████▏  407 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "import sys, os, time\n",
    "now = time.time\n",
    "\n",
    "from utils import pull_ollama_model\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "pull_ollama_model( \"all-minilm\" )\n",
    "\n",
    "local_embeddings = OllamaEmbeddings( model = \"all-minilm\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd67496-84af-4d78-8e3d-1284822eed3a",
   "metadata": {},
   "source": [
    "## Populate document vector database (of pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91002b1f-26e3-488c-b3d3-e751d1f8f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "\n",
    "import fitz, pymupdf\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def pdf_page_to_base64( pdf_path: str, page_number: int ):\n",
    "    zoom_x       = 1.5  # horizontal zoom\n",
    "    zoom_y       = 1.5  # vertical zoom\n",
    "    mat          = pymupdf.Matrix( zoom_x, zoom_y )\n",
    "    pdf_document = fitz.open( pdf_path )\n",
    "    page         = pdf_document.load_page(page_number - 1)  # input is one-indexed\n",
    "    pix          = page.get_pixmap( matrix = mat )\n",
    "    img          = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    buffer       = io.BytesIO()\n",
    "    \n",
    "    img.save( buffer, format=\"PNG\" )\n",
    "\n",
    "    return base64.b64encode( buffer.getvalue() ).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "571f7d54-8795-4b9b-8a8c-66459c4e9d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NO documents to add!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from utils import gen_ID\n",
    "\n",
    "if not path.exists( environ[\"_RAG_PAGE_DESTN\"] ):\n",
    "    makedirs( environ[\"_RAG_PAGE_DESTN\"] )\n",
    "\n",
    "\n",
    "def get_page_meta_key( source, page ):\n",
    "    \"\"\" Generate a (probably not) unique page key with useful data that can also be used for sorting \"\"\"\n",
    "    return str( source ).split('/')[-1].replace(' ','') + '_' + str( page )\n",
    "\n",
    "\n",
    "if int(environ[\"_RAG_DOCDB_REMAIN\"]) > 0:\n",
    "    bgn = now()\n",
    "    docIDs  = [str( gen_ID() ) for _ in range( len(pages) )]\n",
    "    dcmnts  = [str( pg.page_content ) for pg in pages]\n",
    "    metaDt  = deque() # Fast append\n",
    "    d       = 50\n",
    "    readSet = set([])\n",
    "\n",
    "    for i, pg in enumerate( pages ):\n",
    "\n",
    "        id_i  = docIDs[i]\n",
    "        \n",
    "        # Save Text Metadata #\n",
    "        mDct = pg.metadata\n",
    "        mDct['metakey'] = get_page_meta_key( pg.metadata['source'], pg.metadata['page'] )\n",
    "        mDct['docID'  ] = id_i\n",
    "        metaDt.append( mDct )\n",
    "\n",
    "        src = pg.metadata['source']\n",
    "\n",
    "        #  vv- Quick Search -vv     vvvvv--- Long Search ---vvvvvvvv\n",
    "        if (src not in readSet) and (src not in RAGstate.docPaths):\n",
    "            readSet.add( src )\n",
    "            RAGstate.docPaths.append( src )\n",
    "\n",
    "        # Save PDF Page image #\n",
    "        try:\n",
    "            pkl_i = path.join( environ[\"_RAG_PAGE_DESTN\"], f\"{id_i}.pkl\" )\n",
    "            pgPic = pdf_page_to_base64( pg.metadata['source'], pg.metadata['page'] )\n",
    "            \n",
    "            with open( pkl_i, 'wb' ) as f:\n",
    "                RAGstate.pgImgs[ id_i ] = str( pkl_i )\n",
    "                pickle.dump( pgPic, f )\n",
    "        except Exception as e:\n",
    "            print( f\"Could NOT save image ID {id_i}!, {e}\" )\n",
    "\n",
    "        if (i % d == 0):\n",
    "            print( '.', end='', flush = True )\n",
    "    print()\n",
    "\n",
    "    metaDt = list( metaDt )\n",
    "\n",
    "    if len( metaDt ):\n",
    "        collection.add(\n",
    "            ids       = docIDs, \n",
    "            metadatas = metaDt,\n",
    "            documents = dcmnts\n",
    "        )\n",
    "        print( f\"Added {len(dcmnts)} documents in {(now()-bgn)/60.0:.2f} minutes!\" )\n",
    "    else:\n",
    "        print( f\"NO documents to add!\" )\n",
    "\n",
    "RAGstate.save_state( environ[\"_RAG_STATE_PATH\"] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c3459-9cfc-4e17-b249-172bce6cdf2a",
   "metadata": {},
   "source": [
    "# Create vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa5000e0-162c-4f28-9f2b-1e896f42f924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built vector store in 0.0026 seconds!\n"
     ]
    }
   ],
   "source": [
    "bgn = now()\n",
    "vector_store_from_client = Chroma(\n",
    "    client             = persistent_client,\n",
    "    collection_name    = environ[\"_RAG_DOC_DBASE\"],\n",
    "    embedding_function = local_embeddings,\n",
    ")\n",
    "print( f\"Built vector store in {(now()-bgn):.4f} seconds!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7fbc1-f2ba-4ad4-aef4-8bec59bca58d",
   "metadata": {},
   "source": [
    "# Load VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbf9a2dc-a228-43c6-bd29-42247f7c9f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to save 'llava'.\n",
      "This will spew a lot of text on the first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 170370233dd5... 100% ▕████████████████▏ 4.1 GB                         \n",
      "pulling 72d6f08a42f6... 100% ▕████████████████▏ 624 MB                         \n",
      "pulling 43070e2d4e53... 100% ▕████████████████▏  11 KB                         \n",
      "pulling c43332387573... 100% ▕████████████████▏   67 B                         \n",
      "pulling ed11eda7790d... 100% ▕████████████████▏   30 B                         \n",
      "pulling 7c658f9561e5... 100% ▕████████████████▏  564 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "if environ[\"_RAG_RUN_QUERY\"]:\n",
    "    pull_ollama_model( \"llava\" )\n",
    "    \n",
    "    llm = ChatOllama(\n",
    "        model=\"llava\",\n",
    "    )\n",
    "else:\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0461630-e0ca-4dfe-924f-ef19fb70d688",
   "metadata": {},
   "source": [
    "# Setup LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb6c1e36-8c5d-4191-9faa-d07f579578df",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V1\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "if environ[\"_RAG_RUN_QUERY\"]:\n",
    "    # from langchain import hub\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "    from langchain_core.prompts import PromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "    \n",
    "    # Retrieve and generate using the relevant snippets of the blog.\n",
    "    retriever = vector_store_from_client.as_retriever()\n",
    "    \n",
    "    # Instantiation using from_template (recommended)\n",
    "    prompt1 = PromptTemplate( \n",
    "        template = \"\"\"You are an expert assistant capable of interpreting textual information to provide accurate \n",
    "                      and detailed responses. You are provided with the following data:\n",
    "                      Context: {docData}\n",
    "                      Text query: {userQuery}\n",
    "                      Use your understanding of the provided context to generate a response to based on \n",
    "                      relevant, up-to-date information. Ensure your answer is factually accurate, detailed, and leverages academic \n",
    "                      sources where possible. If additional context is required for clarification, request it from the user.\"\"\",\n",
    "        input_variables = [\"docData\",\"userQuery\"],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain1 = (\n",
    "        { \"docData\": retriever | format_docs, \n",
    "          \"userQuery\": RunnablePassthrough()}\n",
    "        | prompt1\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5855a5e1-58d6-4551-b78d-ad7b964c3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_sources( q ):\n",
    "    retrieved_docs = retriever.invoke( q )\n",
    "    generated_ansr = rag_chain1.invoke( q )\n",
    "    return {\n",
    "        'response' : generated_ansr,\n",
    "        'sources'  : retrieved_docs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "444e3c3c-c046-4e1a-9604-b8409a5e9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_local_pages( sourceList ):\n",
    "    global RAGstate\n",
    "    rtnObjs = list()\n",
    "    for source in sourceList:\n",
    "        if source.metadata['docID'] in RAGstate.pgImgs:\n",
    "            pklPath = RAGstate.pgImgs[ source.metadata['docID'] ]\n",
    "            with open( pklPath, 'rb' ) as f:\n",
    "                obj_i = pickle.load( f )\n",
    "                rtnObjs.append( obj_i )\n",
    "        else:\n",
    "            print( f\"No page with ID {source.metadata['docID']}\" )\n",
    "    return rtnObjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f94141ba-2bb3-4dc1-8063-dac1ebc4c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from IPython.display import Image as IPImage\n",
    "from IPython.display import display\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "if environ[\"_RAG_RUN_QUERY\"]:\n",
    "    def deep_doc_ask( q ):\n",
    "        bgn = now()\n",
    "        res = ask_with_sources( q ) \n",
    "        pprint( res['response'] )\n",
    "        print( f\"Initial LLM summary took {now()-bgn:.2f} seconds to process!\" )\n",
    "        \n",
    "        pag = fetch_local_pages( res['sources'] )\n",
    "    \n",
    "        for p in pag:\n",
    "            display( IPImage( data = base64.b64decode( p ) ) )\n",
    "            message = HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": q},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{p}\"},\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "            bgn = now()\n",
    "            response = llm.invoke( [message] )\n",
    "            print( f\"LLM query took {now()-bgn:.2f} seconds to process!\" )\n",
    "            pprint( response.content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a979cf56-711a-4fc9-8961-6fe1ea5dc609",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m environ[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_RAG_RUN_QUERY\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mdeep_doc_ask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow do I estimate the running time of a robot plan based on confidence in the current state estimate?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m, in \u001b[0;36mdeep_doc_ask\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeep_doc_ask\u001b[39m( q ):\n\u001b[1;32m      8\u001b[0m     bgn \u001b[38;5;241m=\u001b[39m now()\n\u001b[0;32m----> 9\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mask_with_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     10\u001b[0m     pprint( res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m] )\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial LLM summary took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnow()\u001b[38;5;241m-\u001b[39mbgn\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds to process!\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m, in \u001b[0;36mask_with_sources\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask_with_sources\u001b[39m( q ):\n\u001b[1;32m      2\u001b[0m     retrieved_docs \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39minvoke( q )\n\u001b[0;32m----> 3\u001b[0m     generated_ansr \u001b[38;5;241m=\u001b[39m \u001b[43mrag_chain1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m : generated_ansr,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msources\u001b[39m\u001b[38;5;124m'\u001b[39m  : retrieved_docs,\n\u001b[1;32m      7\u001b[0m     }\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_ollama/chat_models.py:644\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    639\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 644\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    648\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    649\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    650\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    655\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_ollama/chat_models.py:545\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    544\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    547\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m ChatGenerationChunk(\n\u001b[1;32m    548\u001b[0m                 message\u001b[38;5;241m=\u001b[39mAIMessageChunk(\n\u001b[1;32m    549\u001b[0m                     content\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m                 ),\n\u001b[1;32m    563\u001b[0m             )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_ollama/chat_models.py:527\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    518\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    519\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         tools\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    528\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    529\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[1;32m    530\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    531\u001b[0m         options\u001b[38;5;241m=\u001b[39mOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    532\u001b[0m         keep_alive\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    534\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ollama/_client.py:87\u001b[0m, in \u001b[0;36mClient._stream\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     85\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[1;32m     88\u001b[0m   partial \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[1;32m     89\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m:=\u001b[39m partial\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_models.py:863\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    861\u001b[0m decoder \u001b[38;5;241m=\u001b[39m LineDecoder()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_text():\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m decoder\u001b[38;5;241m.\u001b[39mdecode(text):\n\u001b[1;32m    865\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_models.py:850\u001b[0m, in \u001b[0;36mResponse.iter_text\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    848\u001b[0m chunker \u001b[38;5;241m=\u001b[39m TextChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m byte_content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_bytes():\n\u001b[1;32m    851\u001b[0m         text_content \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(byte_content)\n\u001b[1;32m    852\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(text_content):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_models.py:831\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[1;32m    832\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_models.py:885\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    882\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m    886\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py:127\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_transports/default.py:116\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:367\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:363\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    364\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py:349\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py:341\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 341\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    342\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py:210\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    207\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if environ[\"_RAG_RUN_QUERY\"]:\n",
    "    deep_doc_ask( \"How do I estimate the running time of a robot plan based on confidence in the current state estimate?\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb27bd-e776-48c3-b322-97aae6d7131a",
   "metadata": {},
   "source": [
    "# arXiv Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9823688-6d80-4f4e-be57-5c889d118e14",
   "metadata": {},
   "source": [
    "* When using the legacy APIs (including OAI-PMH, RSS, and the arXiv API), make no more than one request every three seconds, and limit requests to a single connection at a time.\n",
    "    - Suppose 1400 submissions a day, This will take at least 1.2h to download at 1 paper / 3s\n",
    "* [Bulk Data Options](https://info.arxiv.org/help/bulk_data.html)\n",
    "* Do they want donations for mass downloads?\n",
    "* NOT allowed to serve files!\n",
    "* NOT for profit!\n",
    "* CANNOT redirect users to download\n",
    "* [arXiv API Basics](https://info.arxiv.org/help/api/basics.html)\n",
    "* [arXiv API Manual](https://info.arxiv.org/help/api/user-manual.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87af5cf-40b1-4912-ae69-fd7d04565a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1815ace-9adc-4c72-aaaf-d37d6f68ea0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd0bce-e758-4e0b-a1b1-af1e361de2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7d09c-38fe-45de-8dd2-254a9914bd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fb173-ae41-46bb-b930-8cff12c68006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8ec26-bdc9-484e-b88d-e552b7bbee1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
